---
title: "Score Based Generative Models"
summary: Introduce score based generative models
date: 2023-07-27
weight: 2
aliases: ["/score-based"]
tags: ["score-based"]
author: "Pu Zhang"
math: mathjax
---

### Intro

In the ever-evolving landscape of image generation models, diffusion-related approaches have surged in popularity, accompanied by other notable advancements. Models like [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), [Imagen](https://imagen.research.google/), and [DALL-E2](https://openai.com/dall-e-2) have captured attention for their remarkable capabilities. In this post, I will show how score based generative models are related to all these state-of-art generative models. The discussion will encompass a comparative analysis of diffusion models with score-based generative models, showcasing how diffusion techniques can be effectively incorporated into the score-based framework. Additionally, I will explore intriguing possibilities where classifiers can guide the diffusion process, allowing for the generation of image samples based on specific class labels or textual inputs. 

### Generative Model History

In machine learning, there are two main categories of models: generative models and discriminative models. Classifying an image as a dog or a cat is a discriminative model while producing a realistic dog or cat image is a generative model. Common discriminative models include Logistic regression, SVM, random forest or DNN. Common generative models include autoencoders, GANs or diffusion models. 

All generative models follow the same idea that tries to convert a simple distribution, usually a gaussian distribution, into a complex target distribution so that we can transforming the task of sampling from the complex distribution into a simple task of sampling from a gaussian distribution. The target distribution can be, for example, the distribution that our input image set comes from. More formally, let's say our simple gaussian distribution is $X$ and we want to learn a model so that $G(X)$ can approximate the distribution where our input data set comes from. This way, if we sample from $X$, $G(X)$ will also from the same data distribution as our input data.


#### Autoencoder
Autoencoder is one type of generative model that was popular in early days. It contains two parts: a encoder that maps the input data into a latent space and a decoder that maps the latent space back to the output which has the same dimension as the input data. We can view autoencoder as an dimension reduction algorithm as well because the latent space is usually orders of magnetude smaller than the input dimension. The latent space can be seen as the bottleneck of the whole architecture and only the most important information will follow through the architecture and be reconstructed. Vanila autoencoder will encode the input data into arbitrary points in the latent space, which makes sampling from the latent space difficult to generate realistic images once decoded. Because of the limitations, variational autoencoder(VAE) was proposed. 

The idea for VAE is that after encoding the input information into the latent space, it will also tries to ensure that the encoded information follows a standard normal distribution. This is achieved by adding a KL-divergence between the encoded data and a standard normal distribution to the loss. So the loss contains two parts: recontruction loss(MSE as in the vanila case) and a KL-divergence loss. The final encoded data will follow a standard normal distribution. This can ensure that: 1) any two adjacent points in the latent space generate similar results once decoded 2) any point sampled from the standard normal distribution will give us meaningful decoded results. However, the results generated by VAE are usually blurry because it is essentially interpolating different images together.


![autoencoder-arch](images/autoencoder-arch.png)
*<center><font size="3">Autoencoder architecture(by Pu Zhang)</font></center>*


#### GAN
GAN, generative adversarial network, is another popular generative model which is represented as a fight between two components: the generator and the discriminator. It was proposed in 2014. The generator takes random noise as input and tries to produce realistic-looking images. On the other hand, the discriminator tries to tell the fake images generated by the generator from real input images. The discriminator's loss is the BCE loss and the generator's loss tries to maximize the log likelihood for all generated fake samples. Towards the end of the training, both the generator and the discriminator need to be good in order for GAN to generate high quality images. GAN has been shown to produce very realistic images and has been seens as the major breakthrough in the past 10 years in deep learning. But GAN has two main drawbacks: 1) it is very difficult to train the model 2) GAN may suffer from mode collapse problem where the learned images are limited in variaty, namely the model can only generate a small subset of the input data distribution, rather than the whole distribution.

![gan-arch](images/gan-arch.png)
*<center><font size="3">GAN architecture(by Pu Zhang)</font></center>*


#### Denoising Probabilitistic Diffusion Models(DDPM)
Diffusion model was proposed in 2020 and has been shown to generate much better results than GAN. The diffusion model contains the forward process, which gradually destroy the information and the backward process that reverse the whole thing.  To explain how it works, let's start with a 1D exmaple. Let's say that we have input $X$ that follows some complex distribution, we can add gaussian noise $u_t$ to $X$ iteratively following this equation $X_t = \sqrt{1 - p} X_{t-1} + \sqrt{p} \mu_t$, where $\mu \sim \mathcal{N}(0, 1)$ and $p = 0.01$. As you can see that every step's output is a weighted average between the previous step and the gaussian noise. We can write down the final output after $T$ steps as follows:
$$
\begin{aligned}
X_T &= \sqrt{1 - p} X_{T-1} + \sqrt{p} \mu_t \\\
    &= \sqrt{1 - p} (\sqrt{1 - p} X_{T-2} + \sqrt{p} \mu_{T-1}) + \sqrt{p} \mu_t \\\
    &= \ldots \\\
    &= (\sqrt{1 - p})^T X_0 + \sum_{i=0}^{T-1} \sqrt{p}(\sqrt{1 - p})^{i} \mu_{T-i}
\end{aligned}
$$

When we have enough steps, T will approach infinity. And at this time, we will see that the first term above will approach zero($(\sqrt{1 - p})^T \xrightarrow[T \to \infty]{} 0$). For the second term, because we add independent gaussian noise at each step, we can sum up these the variances of all these gaussians together to get the final variance as:
$\sum_{i=0}^{T-1} p(1 - p)^{i} \mu_{T-i} = p \cdot \frac{1 - (1 - p)^T}{1 - (1 - p)} \xrightarrow[T \to \infty]{} 1$.
This means that if we have enough steps, we can convert any distribution $X$ to a standard normal distribution.

![diffusion_1d](images/diffusion-1d.png)
*<center><font size="3">The diffusion process for 1d distribution(by Pu Zhang)</font></center>*

We can extend the same idea to higher dimension. For images of size 512 x 512, we can use diffusion process to gradually turn them into an 512 x 512 isotropic gaussian noise. For this high dimension case, we do the diffusion for each RGB channel of each pixel.

![diffusion_images](images/diffusion-images.png)
*<center><font size="3">The diffusion process for images(by Pu Zhang)</font></center>*


---